<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width" />

    <title>Trabalho de Conclusão de Curso - Lucas Paiolla Forastiere</title>

    <link rel="shortcut icon" href="./res/favicon.png" type="image/png" />

    <link
      href="https://fonts.googleapis.com/css2?family=Nunito:wght@400;700&display=swap"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="./css/page-landing.css" />
  </head>

  <body>
    <center>
      <div class="header">
        <h1>Bem vindx à página do meu TCC</h1>
        <p class="repo-link">
          <a href="https://github.com/Giatroo/TCC" target="_blank">
            <strong>Repositório do projeto</strong></a
          >
        </p>
      </div>

      <div class="proposal">
        <h2>Proposta de Trabalho</h2>

        <p>
          <strong>Introdução do Tema:</strong> Detecção de sarcasmo é um
          importante tópico dentro do processamento de linguagem natural (PLN,
          ou NLP). Esse tema pode ser empregado em vários tipos diferentes de
          sistemas como de mineração de dados, de entendimento da linguagem
          natural ou de diálogo (como chatbots). Entretanto, a detecção de
          sarcasmo é difícil, pois é rara em muitas conversas e, muitas vezes,
          difícil até para nós humanos.
        </p>

        <p></p>

        <p>
          <strong>Objetivos do Trabalho:</strong> No artigo
          <a href="https://arxiv.org/abs/1704.05579" target="_blank"
            >A Large Self-Annotated Corpus for Sarcasm</a
          >, Mikhail Khodak et al. introduzem um conjunto de dados criado para
          treinamento e benchmark de sistemas de detecção de sarcasmo. Nosso
          objetivo é treinar e testar o modelo DeBERTa (Decoding-enhanced Deep
          Bidirectional Transformer with Disentangled Attention) nesse conjunto
          de dados, e comparar resultados com vários outros modelos mais
          clássicos, como o BERT.
        </p>

        <p></p>

        <p>
          <strong>Passos para Atingir os Objetivos:</strong> O cronograma de
          atividades estimadas segue abaixo. Planeja-se inicialmente coletar o
          córpus para realização dos experimentos. Depois disso, vamos fazer um
          modelo base utilizando as bibliotecas
          <a
            href="https://huggingface.co/docs/transformers/index"
            target="_blank"
            ><i>transformers</i></a
          >
          e
          <a href="https://www.sbert.net/" target="_blank"
            ><i>sentence-transformers</i></a
          >. Ao terminar esse baseline, criaremos uma metodologia para
          comparação de modelo (planeja-se comparar com o modelo BERT) e de
          validação dos resultados. Em paralelo com isso, revisarei a literatura
          sobre detecção de sarcamos e uso de transformers para classificação de
          textos. Por fim, para o segundo semestre, o foco será em analisar os
          resultados obtidos utilizando várias métricas (entre elas, métricas
          subjetivas de explicabilidade dos modelos) e também será em finalizar
          a monografia.
        </p>
      </div>

      <div class="future-schedule">
        <h2>Cronograma previsto de tarefas</h2>
        <table class="future-schedule-table">
          <thead>
            <tr>
              <td>Tempo Estimado</td>
              <td>Tarefas</td>
            </tr>
          </thead>

          <tbody>
            <tr>
              <td>14/03 ~ 31/03</td>
              <td>
                <p>
                  Coleta e preparação do córpus textual para experimentos
                  envolvendo detecção de sarcamo;
                </p>
              </td>
            </tr>

            <tr>
              <td>01/04 ~ 30/06</td>
              <td>
                <p>
                  Revisão da literatura envolvendo detecção de sarcamos e
                  classificação de texto usando text embeddings baseados em
                  transformers;
                </p>
              </td>
            </tr>

            <tr>
              <td>01/04 ~ 30/05</td>
              <td>
                <p>
                  Desenvolvimento de um modelo de detecção de sarcasmo usando o
                  DeBERTa;
                </p>
              </td>
            </tr>

            <tr>
              <td>01/06 ~ 31/07</td>
              <td><p>Avaliação experimental usando validação cruzada;</p></td>
            </tr>

            <tr>
              <td>01/08 ~ 31/10</td>
              <td>
                <p>
                  Estudo e discussão dos resultados: medidas objetivas e
                  abordagens subjetivas;
                </p>
              </td>
            </tr>

            <tr>
              <td>01/08 ~ 31/11</td>
              <td><p>Escrita da monografia;</p></td>
            </tr>
          </tbody>
        </table>
      </div>

      <h2>Cronograma de atividades realizadas até então</h2>
      <table class="schedule-table">
        <thead>
          <tr>
            <td>Data/Semana</td>
            <td>Tarefas</td>
          </tr>
        </thead>

        <tbody>
          <tr>
            <td>30/11/2021</td>
            <td>
              <p>
                Troca de emails com o professor orientador demonstrando
                interesse sobre projeto em NLP ou Visão Computacional;
              </p>
            </td>
          </tr>

          <tr>
            <td>17/01/2022</td>
            <td>
              <p>
                Retomada conversa com o orientador para marcar uma reunião
                inicial;
              </p>
            </td>
          </tr>

          <tr>
            <td>04/02/2022</td>
            <td>
              <p>Conversa inicial com apresentação do tema e abordagens;</p>
            </td>
          </tr>

          <tr>
            <td>07/02/2022</td>
            <td>
              <p>
                Leitura do artigo
                <a href="https://arxiv.org/abs/1704.05579" target="_blank"
                  >A Large Self-Annotated Corpus for Sarcasm</a
                >
                pelo aluno para entender melhor o tema e decidir se aceitaria ou
                proporia outro;
              </p>
            </td>
          </tr>

          <tr>
            <td>18/02/2022</td>
            <td>
              <p>Tema aceito;</p>
              <p>
                Envio de material inicial pelo orientador para o entendimento do
                conjunto de dados;
              </p>
            </td>
          </tr>

          <tr>
            <td>04/03/2022</td>
            <td>
              <p>Reunião inicial com a Professora Nina;</p>
              <p>Troca de email com o orientador sobre próximos passos;</p>
            </td>
          </tr>

          <tr>
            <td>19/03/2022</td>
            <td>
              <p>Criação do repositório GitHub;</p>
              <p>Feita uma primeira introdução do TCC;</p>
            </td>
          </tr>

          <tr>
            <td>31/03/2022</td>
            <td>
              <p>Criação do código inicial;</p>
              <p>Criação de um módulo para baixar e descomprimir o dataset;</p>
              <p>
                Criação de um módulo para selecionar um subconjunto dos dados e
                deixá-los em um formato visualizável;
              </p>
            </td>
          </tr>

          <tr>
            <td>04/04/2022</td>
            <td>
              <p>Adicionada uma lista de materiais para estudar;</p>
              <p>
                Início da leitura do artigo
                <a href="https://arxiv.org/abs/1902.02181" target="_blank"
                  >Attention in Natural Langue Processing</a
                >;
              </p>
              <p>Estudo sobre os modelos Transformers;</p>
              <p>Inicio da criação da home-page;</p>
            </td>
          </tr>

          <tr>
            <td>11/04/2022</td>
            <td>
              <p>
                Correção de erros no código de treinamento inicial do modelo;
              </p>
              <p>Criado esse cronograma =);</p>
              <p>Criada sessão da proposta de trabalho no site;</p>
              <p>
                Leitura do paper
                <a href="https://arxiv.org/abs/1706.03762" target="_blank"
                  >Attention Is All You Need</a
                >;
              </p>
              <p>
                Leitura do paper
                <a href="https://arxiv.org/abs/1810.04805" target="_blank"
                  >BERT: Pre-training of Deep Bidirectional Trasnsformers for
                  Language Understanding</a
                >;
              </p>
              <p>
                Termino da leitura do artigo
                <a href="https://arxiv.org/abs/1902.02181" target="_blank"
                  >Attention in Natural Langue Processing</a
                >;
              </p>
              <p>Estudo sobre a biblioteca <i>transformers</i> do Python;</p>
            </td>
          </tr>

          <tr>
            <td>25/04/2022</td>
            <td>
              <p>
                Finalizada a versão parcial desse site e submissão para o
                orientador e para o docente responsável pela disciplina;
              </p>
              <p>
                Leitura do artigo
                <a href="https://arxiv.org/abs/2006.03654" target="_blank"
                  >DeBERTa: Decoding-enhanced BERT with Disentangled
                  Attention</a
                >;
              </p>
            </td>
          </tr>
          <tr>
            <td>02/05/2022</td>
            <td>
              <p>
                Começo da criação de funções para fazer previsões e obter
                métricas para modelos;
              </p>
              <p>
                Início da leitura do artigo <a
                  href="https://dl.acm.org/doi/abs/10.1145/3124420";
                  target="_blank">Automatic Sarcasm Detection: A Survey</a>;
              </p>
            </td>
          </tr>

          <tr>
            <td>09/05/2022</td>
            <td>
              <p>
                Início da revisão do arquivo <i>requirements.txt</i>;
              </p>
              <p>
                Início da transformação do código em notebook para códigos no
                formato <i>.py</i>;
              </p>
              <p>
                Termino da leitura do artigo <a
                  href="https://dl.acm.org/doi/abs/10.1145/3124420";
                  target="_blank">Automatic Sarcasm Detection: A Survey</a>;
              </p>
              <p>
                Primeiro acesso ao laboratório do ICMC via SSH;
              </p>
            </td>
          </tr>

          <tr>
            <td>23/05/2022</td>
            <td>
              <p>
                  Leitura do artigo <a
                  href="https://aclanthology.org/2020.figlang-1.8/";
                  target="_blank">C-Net: Contextual Network for Sarcasm
                  Detection</a>;
              </p>
              <p>
                  Continuação da transformação dos arquivos notebook em arquivos
                  scripts automatizáveis;
              </p>
              <p>
                  Iniciada leitura do artigo <a
                  href="https://link.springer.com/article/10.1007/s00521-020-05102-3";
                  target="_blank">A transformer-based approach to irony and
                  sarcasm detection</a>;
              </p>
            </td>
          </tr>

          <tr>
            <td>30/05/2022</td>
            <td>
              <p>
                  Finalizada leitura do artigo <a
                  href="https://link.springer.com/article/10.1007/s00521-020-05102-3";
                  target="_blank">A transformer-based approach to irony and
                  sarcasm detection</a>;
              </p>
              <p>
                  Criação de datasets pré-carregados e upload deles no drive,
                  de onde eles podem ser baixados diretamente;
              </p>
            </td>
          </tr>

          <tr>
            <td>06/06/2022</td>
            <td>
              <p>
                Treinamento do modelo DeBERTa no laboratório do ICMC;
              </p>
              <p>
                Início da leitura do artigo <a
                  href="https://arxiv.org/abs/1805.06413";
                  target="_blank">CASCADE: Contextual Sarcasm Detection in
                  Online Forums</a>;
              </p>
            </td>
          </tr>

          <tr>
            <td>13/06/2022</td>
            <td>
              <p>
                Criação e refatoração de runners de treinamento e teste para
                serem executados via script dentro do laboratório do ICMC;
              </p>
              <p>
                Continuação do treinamento e teste dos modelos DeBERTa e BERT no
                laboratório;
              </p>
              <p>
                Aparição de dificuldades em relação ao modelo DeBERTa. Por algum
                motivo, após o treinamento do DeBERTa, ele ainda prediz os
                mesmos valores de probabilidade para todos os valores de testes;
              </p>
            </td>
          </tr>

          <tr>
            <td>26/06/2022</td>
            <td>
              <p>
                Tentando resolver o treinamento do modelo e fazer ele realmente
                aprender;
              </p>
            </td>
          </tr>

        </tbody>
      </table>

      <div class="about">
        <h2>Sobre mim</h2>
        <p>
          Eu me chamo Lucas Paiolla Forastiere, sou aluno do Bacharelado em
          Ciência da Computação pelo IME-USP.
        </p>
        <p>
          Minhas áreas de estudo são Machine Learning e Data Science com foco em
          Dados Não-Estruturados (principalmente Texto e Imagens).
        </p>

        <br/>

        <p>lucaspaiolla at usp.br</p>
        <p>
          <a href="https://github.com/Giatroo" target="_blank">GitHub</a>
        </p>
        <p>
          <a href="https://www.linkedin.com/in/lucas-paiolla/" target="_blank"
            >LinkedIn</a
          >
        </p>
      </div>
    </center>
  </body>
</html>
