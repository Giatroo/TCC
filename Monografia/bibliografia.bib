%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%% INSERINDO COMENTÁRIOS EM ARQUIVOS .bib %%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% 1. Como em outros arquivos LaTeX, comentários são iniciados por "%" e,
%    portanto, é preciso usar "\%" para imprimir o caractere "%". Esquecer
%    disso pode gerar erros difíceis de encontrar!
%
% 2. Não é possível inserir comentários dentro de uma entrada, apenas fora.
%
% 3. Não é possível incluir o caractere arroba em um comentário.
%
% 4. Se quiser desabilitar temporariamente um campo ("comentar" o campo),
%    troque seu nome para algum nome inválido, como "author-disable".
%    Se quiser fazer alguma observação permanente dentro de uma entrada,
%    você também pode usar algum nome de campo inválido, como "lembrete",
%    ou usar o campo "annotation", que normalmente é ignorado.
%
% 5. Se quiser desabilitar temporariamente uma entrada inteira ("comentar" a
%    entrada), não basta colocar "%" nas linhas correspondentes por causa do
%    caractere arroba; também é preciso remover a arroba.
%
% 6. Na verdade, as regras não são bem essas, mas segui-las é uma boa ideia:
%    https://tex.stackexchange.com/a/262282


% Esta entrada está comentada, ou seja, não tem efeito: se houvesse uma
% referência a ela no texto, a referência ficaria inválida. Observe que,
% para isso, o caractere arroba foi apagado!
%Book{JW82,
% author    = {Richard A. Johnson and Dean W. Wichern},
% title     = {Applied Multivariate Statistical Analysis},
% publisher = {Prentice-Hall},
% year      = {1983}
%}

% Esta entrada não está em uso em tese.tex, apenas em
% apresentacao.tex, mas isso não é um problema: você pode
% ter um banco de dados com todos os papers que são de seu
% interesse e, em um dado texto, citar apenas alguns deles.
@article{khodak-etal:2017:sarc,
  author    = {Mikhail Khodak and
               Nikunj Saunshi and
               Kiran Vodrahalli},
  title     = {A Large Self-Annotated Corpus for Sarcasm},
  journal   = {CoRR},
  volume    = {abs/1704.05579},
  year      = {2017},
  url       = {http://arxiv.org/abs/1704.05579},
  eprinttype = {arXiv},
  eprint    = {1704.05579},
  timestamp = {Mon, 13 Aug 2018 16:46:44 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KhodakSV17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@online{dicio_sarc,
    author = {Dicio},
    title = {{Sarcasmo} - Dicio, Dicionário Online de Português},
    url = {https://www.dicio.com.br/sarcasmo/},
    urldate={2022-09-19}
}

@online{dicio_irony,
    author = {Dicio},
    title = {{Ironia} - Dicio, Dicionário Online de Português},
    url = {https://www.dicio.com.br/ironia/},
    urldate={2022-09-19}
}

@online{what-does-s-mean,
    author = {Reddit},
    title = {What does /s mean?},
    url = {https://www.reddit.com/r/OutOfTheLoop/comments/1zo2l4/comment/cfvupgz/?utm_source=share&utm_medium=web2x&context=3},
    urldate={2022-11-08}
}

@inproceedings{veale:2010,
    author = {Veale, Tony and Hao, Yanfen},
    title = {Detecting Ironic Intent in Creative Comparisons},
    year = {2010},
    isbn = {9781607506058},
    publisher = {IOS Press},
    address = {NLD},
    abstract = {Irony is an effective but challenging mode of communication that allows a speaker to express sentiment-rich viewpoints with concision, sharpness and humour. Irony is especially common in online documents that express subjective and deeply-felt opinions, and thus represents a significant obstacle to the accurate analysis of sentiment in web texts. In this paper we look at one commonly used framing device for linguistic irony --the simile --to show how irony is often marked in ways that make it computationally feasible to detect. We conduct a very large corpus analysis of web-harvested similes to identify the most interesting characteristics of ironic comparisons, and provide an empirical evaluation of a new algorithm for separating ironic from non-ironic similes.},
    booktitle = {Proceedings of the 2010 Conference on ECAI 2010: 19th European Conference on Artificial Intelligence},
    pages = {765–770},
    numpages = {6}
}

@inproceedings{wallace-etal:2014:ironic-context,
    title = "Humans Require Context to Infer Ironic Intent (so Computers Probably do, too)",
    author = "Wallace, Byron C.  and
      Choe, Do Kook  and
      Kertz, Laura  and
      Charniak, Eugene",
    booktitle = "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P14-2084",
    doi = "10.3115/v1/P14-2084",
    pages = "512--516",
}

@inproceedings{maynard-greenwood:2014:cares,
    title = "Who cares about Sarcastic Tweets? Investigating the Impact of Sarcasm on Sentiment Analysis.",
    author = "Maynard, Diana  and
      Greenwood, Mark",
    booktitle = "Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)",
    month = may,
    year = "2014",
    address = "Reykjavik, Iceland",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2014/pdf/67_Paper.pdf",
    pages = "4238--4243",
    abstract = "Sarcasm is a common phenomenon in social media, and is inherently difficult to analyse, not just automatically but often for humans too. It has an important effect on sentiment, but is usually ignored in social media analysis, because it is considered too tricky to handle. While there exist a few systems which can detect sarcasm, almost no work has been carried out on studying the effect that sarcasm has on sentiment in tweets, and on incorporating this into automatic tools for sentiment analysis. We perform an analysis of the effect of sarcasm scope on the polarity of tweets, and have compiled a number of rules which enable us to improve the accuracy of sentiment analysis when sarcasm is known to be present. We consider in particular the effect of sentiment and sarcasm contained in hashtags, and have developed a hashtag tokeniser for GATE, so that sentiment and sarcasm found within hashtags can be detected more easily. According to our experiments, the hashtag tokenisation achieves 98{\%} Precision, while the sarcasm detection achieved 91{\%} Precision and polarity detection 80{\%}.",
}

@inproceedings{bharti-etal:2015:parsing-sarcasm,
    author={Bharti, Santosh Kumar and Babu, Korra Sathya and Jena, Sanjay Kumar},
    booktitle={2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)},
    title={Parsing-based sarcasm sentiment recognition in Twitter data},
    year={2015},
    volume={},
    number={},
    pages={1373-1380},
    doi={10.1145/2808797.2808910}
}

@article{reyes:2012:from-humor,
    title = {From humor recognition to irony detection: The figurative language of social media},
    journal = {Data \& Knowledge Engineering},
    volume = {74},
    pages = {1-12},
    year = {2012},
    note = {Applications of Natural Language to Information Systems},
    issn = {0169-023X},
    doi = {https://doi.org/10.1016/j.datak.2012.02.005},
    url = {https://www.sciencedirect.com/science/article/pii/S0169023X12000237},
    author = {Antonio Reyes and Paolo Rosso and Davide Buscaldi},
    keywords = {Humor recognition, Irony detection, Natural language processing, Web text analysis},
}

@inproceedings{liebrecht:2013:perfect-solution,
  title={The perfect solution for detecting sarcasm in tweets \#not},
  author={Christine Liebrecht and Florian Kunneman and Antal van den Bosch},
  booktitle={WASSA@NAACL-HLT},
  year={2013}
}

@Article{reyes:2013:multidimensional-approach,
    author={Antonio Reyes and Paolo Rosso and Tony Veale},
    title={A multidimensional approach for detecting irony in Twitter},
    journal={Language Resources and Evaluation},
    year={2013},
    month={Mar},
    day={01},
    volume={47},
    number={1},
    pages={239-268},
    abstract={Irony is a pervasive aspect of many online texts, one made all the more difficult by the absence of face-to-face contact and vocal intonation. As our media increasingly become more social, the problem of irony detection will become even more pressing. We describe here a set of textual features for recognizing irony at a linguistic level, especially in short texts created via social media such as Twitter postings or ``tweets''. Our experiments concern four freely available data sets that were retrieved from Twitter using content words (e.g. ``Toyota'') and user-generated tags (e.g. ``{\#}irony''). We construct a new model of irony detection that is assessed along two dimensions: representativeness and relevance. Initial results are largely positive, and provide valuable insights into the figurative issues facing tasks such as sentiment analysis, assessment of online reputations, or decision making.},
    issn={1574-0218},
    doi={10.1007/s10579-012-9196-x},
    url={https://doi.org/10.1007/s10579-012-9196-x}
}

@inproceedings{barbieri:2014:modelling-sarcasm,
    title = "Modelling Sarcasm in {T}witter, a Novel Approach",
    author = "Barbieri, Francesco  and
      Saggion, Horacio  and
      Ronzano, Francesco",
    booktitle = "Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W14-2609",
    doi = "10.3115/v1/W14-2609",
    pages = "50--58",
}

@inproceedings{joshi:2015:context-incongruity,
    author = {Joshi, Aditya and Sharma, Vinita and Bhattacharyya, Pushpak},
    year = {2015},
    month = {07},
    pages = {},
    title = {Harnessing Context Incongruity for Sarcasm Detection},
    doi = {10.3115/v1/P15-2124}
}

@inproceedings{mishra:2016:harnessing-cognitive,
    title = "Harnessing Cognitive Features for Sarcasm Detection",
    author = "Mishra, Abhijit  and
      Kanojia, Diptesh  and
      Nagar, Seema  and
      Dey, Kuntal  and
      Bhattacharyya, Pushpak",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1104",
    doi = "10.18653/v1/P16-1104",
    pages = "1095--1104",
}

@article{joshi:2017:sarcasm-detection-survey,
    author = {Joshi, Aditya and Bhattacharyya, Pushpak and Carman, Mark J.},
    title = {Automatic Sarcasm Detection: A Survey},
    year = {2017},
    issue_date = {September 2018},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {50},
    number = {5},
    issn = {0360-0300},
    url = {https://doi.org/10.1145/3124420},
    doi = {10.1145/3124420},
    abstract = {Automatic sarcasm detection is the task of predicting sarcasm in text. This is a crucial step to sentiment analysis, considering prevalence and challenges of sarcasm in sentiment-bearing text. Beginning with an approach that used speech-based features, automatic sarcasm detection has witnessed great interest from the sentiment analysis community. This article is a compilation of past work in automatic sarcasm detection. We observe three milestones in the research so far: semi-supervised pattern extraction to identify implicit sentiment, use of hashtag-based supervision, and incorporation of context beyond target text. In this article, we describe datasets, approaches, trends, and issues in sarcasm detection. We also discuss representative performance values, describe shared tasks, and provide pointers to future work, as given in prior works. In terms of resources to understand the state-of-the-art, the survey presents several useful illustrations—most prominently, a table that summarizes past papers along different dimensions such as the types of features, annotation techniques, and datasets used.},
    journal = {ACM Comput. Surv.},
    month = {sep},
    articleno = {73},
    numpages = {22},
    keywords = {opinion, Sarcasm, sentiment analysis, sarcasm detection, sentiment}
}

@article{yaghoobian:2021:sarcasm-detection-comparative-study,
  doi = {10.48550/ARXIV.2107.02276},
  url = {https://arxiv.org/abs/2107.02276},
  author = {Yaghoobian, Hamed and Arabnia, Hamid R. and Rasheed, Khaled},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Sarcasm Detection: A Comparative Study},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{riloff:2013:sarcasm-constract,
  title={Sarcasm as Contrast between a Positive Sentiment and Negative Situation},
  author={Ellen Riloff and Ashequl Qadir and Praful Surve and Lalindra De Silva and Nathan Gilbert and Ruihong Huang},
  booktitle={EMNLP},
  year={2013}
}

@inproceedings{joshi-etal:2016:harnessing,
    title = "Harnessing Sequence Labeling for Sarcasm Detection in Dialogue from {TV} Series {`}{F}riends{'}",
    author = "Joshi, Aditya  and
      Tripathi, Vaibhav  and
      Bhattacharyya, Pushpak  and
      Carman, Mark J.",
    booktitle = "Proceedings of the 20th {SIGNLL} Conference on Computational Natural Language Learning",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K16-1015",
    doi = "10.18653/v1/K16-1015",
    pages = "146--155",
}

@inproceedings{liu-etal:2014:imbalanced-classification,
    author = {Liu, Peng and Chen, Wei and Ou, Gaoyan and Wang, Tengjiao and Yang, Dongqing and Lei, Kai},
    year = {2014},
    month = {06},
    pages = {459-471},
    title = {Sarcasm Detection in Social Media Based on Imbalanced Classification},
    isbn = {978-3-319-08009-3},
    doi = {10.1007/978-3-319-08010-9_49}
}

@inproceedings{ghosh-veale:2016:fracking-sarcasm-nn,
    title = "Fracking Sarcasm using Neural Network",
    author = "Ghosh, Aniruddha  and
      Veale, Tony",
    booktitle = "Proceedings of the 7th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W16-0425",
    doi = "10.18653/v1/W16-0425",
    pages = "161--169",
}

@inproceedings{van-hee-etal:2018:semeval,
    title = "{S}em{E}val-2018 Task 3: Irony Detection in {E}nglish Tweets",
    author = "Van Hee, Cynthia  and
      Lefever, Els  and
      Hoste, V{\'e}ronique",
    booktitle = "Proceedings of the 12th International Workshop on Semantic Evaluation",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S18-1005",
    doi = "10.18653/v1/S18-1005",
    pages = "39--50",
    abstract = "This paper presents the first shared task on irony detection: given a tweet, automatic natural language processing systems should determine whether the tweet is ironic (Task A) and which type of irony (if any) is expressed (Task B). The ironic tweets were collected using irony-related hashtags (i.e. {\#}irony, {\#}sarcasm, {\#}not) and were subsequently manually annotated to minimise the amount of noise in the corpus. Prior to distributing the data, hashtags that were used to collect the tweets were removed from the corpus. For both tasks, a training corpus of 3,834 tweets was provided, as well as a test set containing 784 tweets. Our shared tasks received submissions from 43 teams for the binary classification Task A and from 31 teams for the multiclass Task B. The highest classification scores obtained for both subtasks are respectively F1= 0.71 and F1= 0.51 and demonstrate that fine-grained irony classification is much more challenging than binary irony detection.",
}

@inproceedings{hazarika-etal:2018:cascade,
    title = "{CASCADE}: Contextual Sarcasm Detection in Online Discussion Forums",
    author = "Hazarika, Devamanyu  and
      Poria, Soujanya  and
      Gorantla, Sruthi  and
      Cambria, Erik  and
      Zimmermann, Roger  and
      Mihalcea, Rada",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/C18-1156",
    pages = "1837--1848",
    abstract = "The literature in automated sarcasm detection has mainly focused on lexical-, syntactic- and semantic-level analysis of text. However, a sarcastic sentence can be expressed with contextual presumptions, background and commonsense knowledge. In this paper, we propose a ContextuAl SarCasm DEtector (CASCADE), which adopts a hybrid approach of both content- and context-driven modeling for sarcasm detection in online social media discussions. For the latter, CASCADE aims at extracting contextual information from the discourse of a discussion thread. Also, since the sarcastic nature and form of expression can vary from person to person, CASCADE utilizes user embeddings that encode stylometric and personality features of users. When used along with content-based feature extractors such as convolutional neural networks, we see a significant boost in the classification performance on a large Reddit corpus.",
}

@inproceedings{wang-etal:2015:context-twitter,
    author="Wang, Zelin
    and Wu, Zhijian
    and Wang, Ruimin
    and Ren, Yafeng",
    editor="Wang, Jianyong
    and Cellary, Wojciech
    and Wang, Dingding
    and Wang, Hua
    and Chen, Shu-Ching
    and Li, Tao
    and Zhang, Yanchun",
    title="Twitter Sarcasm Detection Exploiting a Context-Based Model",
    booktitle="Web Information Systems Engineering -- WISE 2015",
    year="2015",
    publisher="Springer International Publishing",
    address="Cham",
    pages="77--91",
    abstract="Automatically detecting sarcasm in twitter is a challenging task because sarcasm transforms the polarity of an apparently positive or negative utterance into its opposite. Previous work focus on feature modeling of the single tweet, which limit the performance of the task. These methods did not leverage contextual information regarding the author or the tweet to improve the performance of sarcasm detection. However, tweets are filtered through streams of posts, so that a wider context, e.g. a conversation or topic, is always available. In this paper, we compared sarcastic utterances in twitter to utterances that express positive or negative attitudes without sarcasm. The sarcasm detection problem is modeled as a sequential classification task over a tweet and his contextual information. A Markovian formulation of the Support Vector Machine discriminative model as embodied by the SVM-HMM algorithm has been employed to assign the category label to entire sequence. Experimental results show that sequential classification effectively embodied evidence about the context information and is able to reach a relative increment in detection performance.",
    isbn="978-3-319-26190-4",
}

@inproceedings{ghosh:2018:sarcasm-conversation-context,
    author = {Ghosh, Debanjan and Fabbri, Alexander R. and Muresan, Smaranda},
    title = "{Sarcasm Analysis Using Conversation Context}",
    journal = {Computational Linguistics},
    volume = {44},
    number = {4},
    pages = {755-792},
    year = {2018},
    month = {12},
    abstract = "{Computational models for sarcasm detection have often relied on the content of utterances in isolation. However, the speaker’s sarcastic intent is not always apparent without additional context. Focusing on social media discussions, we investigate three issues: (1) does modeling conversation context help in sarcasm detection? (2) can we identify what part of conversation context triggered the sarcastic reply? and (3) given a sarcastic post that contains multiple sentences, can we identify the specific sentence that is sarcastic? To address the first issue, we investigate several types of Long Short-Term Memory (LSTM) networks that can model both the conversation context and the current turn. We show that LSTM networks with sentence-level attention on context and current turn, as well as the conditional LSTM network, outperform the LSTM model that reads only the current turn. As conversation context, we consider the prior turn, the succeeding turn, or both. Our computational models are tested on two types of social media platforms: Twitter and discussion forums. We discuss several differences between these data sets, ranging from their size to the nature of the gold-label annotations. To address the latter two issues, we present a qualitative analysis of the attention weights produced by the LSTM models (with attention) and discuss the results compared with human performance on the two tasks.}",
    issn = {0891-2017},
    doi = {10.1162/coli_a_00336},
    url = {https://doi.org/10.1162/coli\_a\_00336},
    eprint = {https://direct.mit.edu/coli/article-pdf/44/4/755/1809923/coli\_a\_00336.pdf},
}

@inproceedings{jena-etal:2020:cnet,
    title = "{C}-Net: Contextual Network for Sarcasm Detection",
    author = "Kumar Jena, Amit  and
      Sinha, Aman  and
      Agarwal, Rohit",
    booktitle = "Proceedings of the Second Workshop on Figurative Language Processing",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.figlang-1.8",
    doi = "10.18653/v1/2020.figlang-1.8",
    pages = "61--66",
    abstract = "Automatic Sarcasm Detection in conversations is a difficult and tricky task. Classifying an utterance as sarcastic or not in isolation can be futile since most of the time the sarcastic nature of a sentence heavily relies on its context. This paper presents our proposed model, C-Net, which takes contextual information of a sentence in a sequential manner to classify it as sarcastic or non-sarcastic. Our model showcases competitive performance in the Sarcasm Detection shared task organised on CodaLab and achieved 75.0\% F1-score on the Twitter dataset and 66.3\% F1-score on Reddit dataset.",
}

@article{kolchinski-potts:2018:representing-social-media-users,
  author    = {Y. Alex Kolchinski and Christopher Potts},
  title     = {Representing Social Media Users for Sarcasm Detection},
  journal   = {CoRR},
  volume    = {abs/1808.08470},
  year      = {2018},
  url       = {http://arxiv.org/abs/1808.08470},
  eprinttype = {arXiv},
  eprint    = {1808.08470},
}

@inproceedings{pennington-etal:2014:glove,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1162",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543",
}

@misc{peters-etal:2018:elmo,
  doi = {10.48550/ARXIV.1802.05365},
  url = {https://arxiv.org/abs/1802.05365},
  author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Deep contextualized word representations},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{mikolov-etal:2013:word2vec,
  doi = {10.48550/ARXIV.1301.3781},
  url = {https://arxiv.org/abs/1301.3781},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Efficient Estimation of Word Representations in Vector Space},
  publisher = {arXiv},
  year = {2013},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{bojanowski-etal:2016:fasttext,
  doi = {10.48550/ARXIV.1607.04606},
  url = {https://arxiv.org/abs/1607.04606},
  author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Enriching Word Vectors with Subword Information},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{devlin-etal:2018:bert,
  doi = {10.48550/ARXIV.1810.04805},
  url = {https://arxiv.org/abs/1810.04805},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{liu-etal:2019:roberta,
  doi = {10.48550/ARXIV.1907.11692},
  url = {https://arxiv.org/abs/1907.11692},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{he-etal:2020:deberta,
  doi = {10.48550/ARXIV.2006.03654},
  url = {https://arxiv.org/abs/2006.03654},
  author = {He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2; I.7, cs.CL, cs.GL},
  title = {DeBERTa: Decoding-enhanced BERT with Disentangled Attention},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{he-etal:2021:debertav3,
  author    = {Pengcheng He and
               Jianfeng Gao and
               Weizhu Chen},
  title     = {DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with
               Gradient-Disentangled Embedding Sharing},
  journal   = {CoRR},
  volume    = {abs/2111.09543},
  year      = {2021},
  url       = {https://arxiv.org/abs/2111.09543},
  eprinttype = {arXiv},
  eprint    = {2111.09543},
  timestamp = {Mon, 22 Nov 2021 16:44:07 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2111-09543.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{brown-etal:2020:gpt3,
  doi = {10.48550/ARXIV.2005.14165},
  url = {https://arxiv.org/abs/2005.14165},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Language Models are Few-Shot Learners},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{yang-etal:2019:xlnet,
  doi = {10.48550/ARXIV.1906.08237},
  url = {https://arxiv.org/abs/1906.08237},
  author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  publisher = {arXiv},
  year = {2019},
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{potamias-etal:2020:transform-sarcasm,
    author={Potamias, Rolandos Alexandros
    and Siolas, Georgios
    and Stafylopatis, Andreas -. Georgios},
    title={A transformer-based approach to irony and sarcasm detection},
    journal={Neural Computing and Applications},
    year={2020},
    month={Dec},
    day={01},
    volume={32},
    number={23},
    pages={17309-17320},
    abstract={Figurative language (FL) seems ubiquitous in all social media discussion forums and chats, posing extra challenges to sentiment analysis endeavors. Identification of FL schemas in short texts remains largely an unresolved issue in the broader field of natural language processing, mainly due to their contradictory and metaphorical meaning content. The main FL expression forms are sarcasm, irony and metaphor. In the present paper, we employ advanced deep learning methodologies to tackle the problem of identifying the aforementioned FL forms. Significantly extending our previous work (Potamias et al., in: International conference on engineering applications of neural networks, Springer, Berlin, pp 164--175, 2019), we propose a neural network methodology that builds on a recently proposed pre-trained transformer-based network architecture which is further enhanced with the employment and devise of a recurrent convolutional neural network. With this setup, data preprocessing is kept in minimum. The performance of the devised hybrid neural architecture is tested on four benchmark datasets, and contrasted with other relevant state-of-the-art methodologies and systems. Results demonstrate that the proposed methodology achieves state-of-the-art performance under all benchmark datasets, outperforming, even by a large margin, all other methodologies and published studies.},
    issn={1433-3058},
    doi={10.1007/s00521-020-05102-3},
    url={https://doi.org/10.1007/s00521-020-05102-3}
}

@article{clark-etal:2020:electra,
  author    = {Kevin Clark and
               Minh{-}Thang Luong and
               Quoc V. Le and
               Christopher D. Manning},
  title     = {{ELECTRA:} Pre-training Text Encoders as Discriminators Rather Than
               Generators},
  journal   = {CoRR},
  volume    = {abs/2003.10555},
  year      = {2020},
  url       = {https://arxiv.org/abs/2003.10555},
  eprinttype = {arXiv},
  eprint    = {2003.10555},
  timestamp = {Wed, 01 Apr 2020 17:39:11 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2003-10555.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{hochreiter-schmidhuber:1997:lstm,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    year = {1997},
    month = {12},
    pages = {1735-80},
    title = {Long Short-term Memory},
    volume = {9},
    journal = {Neural computation},
    doi = {10.1162/neco.1997.9.8.1735}
}

@article{galassi:2021:attention-in-nlp,
	doi = {10.1109/tnnls.2020.3019893},
	url = {https://doi.org/10.1109%2Ftnnls.2020.3019893},
	year = 2021,
	month = {oct},
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
	volume = {32},
	number = {10},
	pages = {4291--4308},
	author = {Andrea Galassi and Marco Lippi and Paolo Torroni},
	title = {Attention in Natural Language Processing},
	journal = {{IEEE} Transactions on Neural Networks and Learning Systems}
}

@misc{vaswani-etal:2017:attention-is-all-you-need,
  doi = {10.48550/ARXIV.1706.03762},
  url = {https://arxiv.org/abs/1706.03762},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Attention Is All You Need},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{schuster-paliwal:1997:birnns,
    author = {Schuster, Mike and Paliwal, Kuldip},
    year = {1997},
    month = {12},
    pages = {2673 - 2681},
    title = {Bidirectional recurrent neural networks},
    volume = {45},
    journal = {Signal Processing, IEEE Transactions on},
    doi = {10.1109/78.650093}
}

@misc{bahdanau-etal:2014:attention-mechanism,
  doi = {10.48550/ARXIV.1409.0473},
  url = {https://arxiv.org/abs/1409.0473},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Neural Machine Translation by Jointly Learning to Align and Translate},
  publisher = {arXiv},
  year = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{cho-etal:2014:rnn-encoder-decoder,
  doi = {10.48550/ARXIV.1406.1078},
  url = {https://arxiv.org/abs/1406.1078},
  author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation},
  publisher = {arXiv},
  year = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
